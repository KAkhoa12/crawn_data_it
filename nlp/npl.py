import pandas as pd
import spacy
import re
import ast
from tqdm import tqdm

# Load mÃ´ hÃ¬nh NLP tiáº¿ng Anh
nlp = spacy.load("en_core_web_sm")

# Äá»c file CSV
df = pd.read_csv("itviec_jobs_undetected.csv")  # Ä‘á»•i tÃªn file náº¿u cáº§n

# Danh sÃ¡ch tá»« khÃ´ng cÃ³ nghÄ©a vÃ  sai chÃ­nh táº£ cáº§n loáº¡i bá»
MEANINGLESS_WORDS = {
    # Tá»« khÃ´ng cÃ³ nghÄ©a
    'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
    'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall',
    'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
    'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their',
    'what', 'when', 'where', 'why', 'how', 'who', 'which', 'whose',
    'if', 'then', 'else', 'than', 'as', 'so', 'too', 'very', 'much', 'many', 'more', 'most',
    'some', 'any', 'all', 'each', 'every', 'no', 'not', 'only', 'just', 'also', 'even',
    'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further', 'then', 'once',
    # Tá»« thÆ°á»ng gáº·p khÃ´ng cÃ³ Ã½ nghÄ©a trong context tuyá»ƒn dá»¥ng
    'job', 'work', 'company', 'team', 'role', 'position', 'candidate', 'applicant',
    'good', 'great', 'excellent', 'strong', 'solid', 'proven', 'successful',
    # Tá»« sai chÃ­nh táº£ phá»• biáº¿n
    'teh', 'adn', 'nad', 'hte', 'taht', 'thier', 'recieve', 'seperate', 'definately',
    'occured', 'begining', 'untill', 'writting', 'comming', 'runing', 'geting',
    # Tá»« quÃ¡ ngáº¯n (1-2 kÃ½ tá»±)
    'a', 'i', 'to', 'of', 'in', 'it', 'is', 'be', 'as', 'at', 'so', 'we', 'he', 'by', 'or', 'on', 'do', 'if', 'me', 'my', 'up', 'an', 'go', 'no', 'us', 'am', 'ok'
}

# LÃ m sáº¡ch vÄƒn báº£n (giá»¯ kÃ½ tá»± tiáº¿ng Viá»‡t)
def clean_text(text):
    if pd.isnull(text): return ""
    if isinstance(text, list):
        text = ' '.join(map(str, text))
    text = str(text)
    text = text.lower()
    text = re.sub(r'\n|\r', ' ', text)
    # Giá»¯ kÃ½ tá»± tiáº¿ng Viá»‡t: a-z, A-Z, 0-9, vÃ  kÃ½ tá»± cÃ³ dáº¥u tiáº¿ng Viá»‡t
    text = re.sub(r'[^a-zA-Z0-9\s\u00C0-\u1EF9]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Lá»c tá»« cÃ³ nghÄ©a
def filter_meaningful_words(words):
    """Lá»c bá» tá»« khÃ´ng cÃ³ nghÄ©a vÃ  sai chÃ­nh táº£"""
    filtered = []
    for word in words:
        word_clean = word.lower().strip()
        # Bá» tá»« quÃ¡ ngáº¯n (< 3 kÃ½ tá»±) hoáº·c quÃ¡ dÃ i (> 30 kÃ½ tá»±)
        if len(word_clean) < 3 or len(word_clean) > 30:
            continue
        # Bá» tá»« trong danh sÃ¡ch meaningless
        if word_clean in MEANINGLESS_WORDS:
            continue
        # Bá» tá»« chá»‰ chá»©a sá»‘
        if word_clean.isdigit():
            continue
        # Bá» tá»« cÃ³ quÃ¡ nhiá»u kÃ½ tá»± láº·p láº¡i (nhÆ° 'aaaa', 'xxxx')
        if len(set(word_clean)) <= 2 and len(word_clean) > 3:
            continue
        filtered.append(word)
    return filtered

# TrÃ­ch 4 cá»¥m tá»« báº±ng spaCy vá»›i bá»™ lá»c
def extract_clusters(text):
    doc = nlp(text)
    primary, secondary, adjectives, adverbs = set(), set(), set(), set()
    for token in doc:
        # Bá» qua token náº¿u lÃ  stop word hoáº·c khÃ´ng pháº£i chá»¯ cÃ¡i
        if token.is_stop or not token.is_alpha:
            continue

        word = token.lemma_.lower()

        if token.pos_ in ['NOUN', 'PROPN']:
            primary.add(word)
        elif token.pos_ == 'VERB':
            secondary.add(word)
        elif token.pos_ == 'ADJ':
            adjectives.add(word)
        elif token.pos_ == 'ADV':
            adverbs.add(word)

    # Ãp dá»¥ng bá»™ lá»c cho táº¥t cáº£ cÃ¡c cá»¥m
    primary = set(filter_meaningful_words(primary))
    secondary = set(filter_meaningful_words(secondary))
    adjectives = set(filter_meaningful_words(adjectives))
    adverbs = set(filter_meaningful_words(adverbs))

    return primary, secondary, adjectives, adverbs

# TrÃ­ch xuáº¥t tá»« tiáº¿ng Viá»‡t báº±ng underthesea vá»›i cáº£i thiá»‡n
def extract_vi_clusters(text):
    """TrÃ­ch xuáº¥t tÃ­nh tá»« vÃ  tráº¡ng tá»« tiáº¿ng Viá»‡t báº±ng underthesea"""
    from underthesea import pos_tag, word_tokenize
    import re

    if not text.strip():
        return set(), set(), set(), set()

    try:
        # LÃ m sáº¡ch text trÆ°á»›c khi xá»­ lÃ½
        text_clean = re.sub(r'[^\w\s\u00C0-\u1EF9]', ' ', text)  # Giá»¯ kÃ½ tá»± tiáº¿ng Viá»‡t
        text_clean = re.sub(r'\s+', ' ', text_clean).strip()

        if not text_clean:
            return set(), set(), set(), set()

        # Word tokenization trÆ°á»›c Ä‘á»ƒ trÃ¡nh cáº¯t tá»« sai
        words = word_tokenize(text_clean)

        # POS tagging tá»«ng tá»«
        primary, secondary, adjectives, adverbs = set(), set(), set(), set()

        # Xá»­ lÃ½ tá»«ng cÃ¢u Ä‘á»ƒ trÃ¡nh lá»—i
        sentences = re.split(r'[.!?]+', text_clean)

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            try:
                pos_tags = pos_tag(sentence)

                for word, tag in pos_tags:
                    word_clean = word.lower().strip()

                    # Kiá»ƒm tra tá»« há»£p lá»‡
                    if len(word_clean) < 2:
                        continue

                    # Kiá»ƒm tra cÃ³ kÃ½ tá»± tiáº¿ng Viá»‡t hoáº·c tiáº¿ng Anh
                    if not re.match(r'^[a-zA-ZÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+$', word_clean):
                        continue

                    # Debug: In ra Ä‘á»ƒ kiá»ƒm tra
                    # print(f"Word: {word_clean}, Tag: {tag}")

                    # PhÃ¢n loáº¡i theo POS tag cá»§a underthesea
                    if tag in ['N', 'Np', 'Ny', 'Nc']:  # Danh tá»«
                        primary.add(word_clean)
                    elif tag in ['V', 'Vb', 'Vy', 'Va']:  # Äá»™ng tá»«
                        secondary.add(word_clean)
                    elif tag in ['A', 'Ab', 'Aa']:  # TÃ­nh tá»«
                        adjectives.add(word_clean)
                    elif tag in ['R', 'Rb', 'Ra']:  # Tráº¡ng tá»«
                        adverbs.add(word_clean)

            except Exception as e:
                print(f"Error processing sentence: {sentence[:50]}... - {e}")
                continue

        # KhÃ´ng Ã¡p dá»¥ng bá»™ lá»c quÃ¡ nghiÃªm ngáº·t cho tiáº¿ng Viá»‡t
        # Chá»‰ lá»c tá»« quÃ¡ ngáº¯n
        primary = {w for w in primary if len(w) >= 2}
        secondary = {w for w in secondary if len(w) >= 2}
        adjectives = {w for w in adjectives if len(w) >= 2}
        adverbs = {w for w in adverbs if len(w) >= 2}

        return primary, secondary, adjectives, adverbs

    except Exception as e:
        print(f"Error in Vietnamese POS tagging: {e}")
        return set(), set(), set(), set()

# Chuyá»ƒn Ä‘á»•i skills thÃ nh primary_skills (giá»¯ nguyÃªn khÃ´ng lá»c)
def process_skills_to_primary(skills_raw):
    """Chuyá»ƒn Ä‘á»•i skills thÃ nh primary_skills giá»‘ng y chang"""
    if isinstance(skills_raw, str) and skills_raw.startswith("["):
        try:
            skills = ast.literal_eval(skills_raw)
        except:
            skills = []
    elif isinstance(skills_raw, list):
        skills = skills_raw
    else:
        skills = [str(skills_raw)] if skills_raw else []

    # Chá»‰ lÃ m sáº¡ch cÆ¡ báº£n, khÃ´ng Ã¡p dá»¥ng bá»™ lá»c nghiÃªm ngáº·t
    cleaned_skills = []
    for skill in skills:
        if isinstance(skill, str):
            skill_clean = skill.strip()
            if skill_clean:  # Chá»‰ bá» skill rá»—ng
                cleaned_skills.append(skill_clean)

    return cleaned_skills

# PhÃ¢n tÃ¡ch vÄƒn báº£n tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t
def separate_sentences_by_lang(text):
    """PhÃ¢n tÃ¡ch cÃ¢u tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t"""
    import re

    # TÃ¡ch cÃ¢u
    sentences = re.split(r'[.!?]+', text)

    en_sentences = []
    vi_sentences = []

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        # Äáº¿m kÃ½ tá»± tiáº¿ng Viá»‡t (cÃ³ dáº¥u)
        vietnamese_chars = len(re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', sentence.lower()))
        total_chars = len(re.findall(r'[a-zA-ZÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', sentence.lower()))

        if total_chars > 0:
            vietnamese_ratio = vietnamese_chars / total_chars
            if vietnamese_ratio > 0.1:  # Náº¿u > 10% kÃ½ tá»± tiáº¿ng Viá»‡t
                vi_sentences.append(sentence)
            else:
                en_sentences.append(sentence)

    return ' '.join(en_sentences), ' '.join(vi_sentences)

# Gá»™p káº¿t quáº£ tá»« tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t
def merge_clusters(en_clusters, vi_clusters):
    """Gá»™p káº¿t quáº£ tá»« tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t"""
    en_primary, en_secondary, en_adj, en_adv = en_clusters
    vi_primary, vi_secondary, vi_adj, vi_adv = vi_clusters

    return {
        "primary_skills": list(set(en_primary) | set(vi_primary)),
        "secondary_skills": list(set(en_secondary) | set(vi_secondary)),
        "adjectives": list(set(en_adj) | set(vi_adj)),
        "adverbs": list(set(en_adv) | set(vi_adv))
    }

# HÃ m xá»­ lÃ½ tá»«ng dÃ²ng vá»›i debug
def process_row(row):
    desc = clean_text(row.get("description", ""))
    reqs = clean_text(row.get("requirements", ""))
    full_text = f"{desc} {reqs}"

    # Debug: In ra text Ä‘á»ƒ kiá»ƒm tra
    job_title = row.get("title", "Unknown")
    print(f"\nğŸ” Processing: {job_title}")
    print(f"ğŸ“ Full text sample: {full_text[:100]}...")

    # PhÃ¢n tÃ¡ch tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t
    en_part, vi_part = separate_sentences_by_lang(full_text)

    print(f"ğŸ‡ºğŸ‡¸ English part: {en_part[:50]}..." if en_part else "ğŸ‡ºğŸ‡¸ English part: (empty)")
    print(f"ğŸ‡»ğŸ‡³ Vietnamese part: {vi_part[:50]}..." if vi_part else "ğŸ‡»ğŸ‡³ Vietnamese part: (empty)")

    # TrÃ­ch xuáº¥t tá»« tiáº¿ng Anh
    en_clusters = extract_clusters(en_part) if en_part.strip() else (set(), set(), set(), set())

    # TrÃ­ch xuáº¥t tá»« tiáº¿ng Viá»‡t
    vi_clusters = extract_vi_clusters(vi_part) if vi_part.strip() else (set(), set(), set(), set())

    print(f"ğŸ‡ºğŸ‡¸ EN adjectives: {list(en_clusters[2])[:5]}")
    print(f"ğŸ‡ºğŸ‡¸ EN adverbs: {list(en_clusters[3])[:5]}")
    print(f"ğŸ‡»ğŸ‡³ VI adjectives: {list(vi_clusters[2])[:5]}")
    print(f"ğŸ‡»ğŸ‡³ VI adverbs: {list(vi_clusters[3])[:5]}")

    # Gá»™p káº¿t quáº£
    text_analysis = merge_clusters(en_clusters, vi_clusters)

    # primary_skills sáº½ giá»‘ng y chang cá»™t skills
    primary_skills = process_skills_to_primary(row.get("skills", ""))

    return pd.Series({
        "primary_skills": primary_skills,
        "secondary_skills": text_analysis["secondary_skills"],
        "adjectives": text_analysis["adjectives"],
        "adverbs": text_analysis["adverbs"]
    })

# Ãp dá»¥ng xá»­ lÃ½
tqdm.pandas()
processed = df.progress_apply(process_row, axis=1)

# GhÃ©p káº¿t quáº£ vÃ o DataFrame vÃ  xÃ³a cá»™t skills Ä‘á»ƒ khÃ´ng bá»‹ dÆ°
df = pd.concat([df.drop(columns=["description", "requirements", "skills"], errors="ignore"), processed], axis=1)

# Test underthesea trÆ°á»›c khi lÆ°u
def test_underthesea():
    """Test underthesea vá»›i má»™t cÃ¢u tiáº¿ng Viá»‡t Ä‘Æ¡n giáº£n"""
    from underthesea import pos_tag

    test_text = "CÃ´ng viá»‡c nÃ y ráº¥t thÃº vá»‹ vÃ  háº¥p dáº«n. TÃ´i lÃ m viá»‡c chÄƒm chá»‰ vÃ  nhanh chÃ³ng."
    print(f"\nğŸ§ª Testing underthesea with: {test_text}")

    try:
        pos_result = pos_tag(test_text)
        print(f"POS result: {pos_result}")

        adjectives = []
        adverbs = []

        for word, tag in pos_result:
            if tag in ['A', 'Ab', 'Aa']:
                adjectives.append(f"{word}({tag})")
            elif tag in ['R', 'Rb', 'Ra']:
                adverbs.append(f"{word}({tag})")

        print(f"Adjectives found: {adjectives}")
        print(f"Adverbs found: {adverbs}")

    except Exception as e:
        print(f"Error in test: {e}")

# Cháº¡y test
test_underthesea()

# LÆ°u káº¿t quáº£
df.to_csv("job_postings_processed.csv", index=False)
print("âœ… ÄÃ£ xá»­ lÃ½ xong vÃ  lÆ°u vÃ o job_postings_processed.csv")
